{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'E:\\python-test\\study-python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "loaded_dataset = load_dataset(r\"E:\\python-test\\DATASETS\\zxbsmkwebnovel_cn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '有其主必有其仆，咱们张大少爷的话还没有说完，张石头的口水就已经流出了嘴角，张大少爷又趁热打铁，鼓动道：“你想想，熊瑚那个丫鬟秀儿长得有多漂亮啊？你如果一辈子窝在这个临清城里，有没有可能讨到这么漂亮的媳妇？这么好的机会放在面前，难道你想错过，以后就只抱着一个黄脸婆过下半辈子？”\\n张大少爷的话句句都打中要害，二十出头还没讨到老婆的张石头难免动心无比，不过仔细考虑了一下后，张石头还是哭丧着脸说道：“少爷，不是小的拦着你，只是这事可不是开玩笑，你从小就没离开过临清城，忽然之间悄悄离开家进京，老爷和十一位夫人怎么可能放心得下？只怕你前脚出了城，老爷后脚就派人把你抓回家了。而且还有一点，那只母老虎熊瑚实在太凶了，我们进了京城就算能找得到她，只怕少爷你也没办法把她娶回家啊？”\\n“怕什么？只要功夫深，铁棒磨成针，咱们只要再见到熊瑚那只母老虎，就肯定有办法把她连人带丫鬟一起弄回临清。”早有打算却不能明言的张大少爷随口瞎扯，又皱眉道：“至于老头子这边，是得防着他派人来抓我们，否则他回来发现少爷我不在家，肯定要到处去找。”琢磨到这里，张大少爷一拍大腿，喜道：“对了，有办法让老头子抓不到咱们了。”\\n“什么办法？”张石头赶紧问道。张大少爷奸笑道：“别急，到时候你就知道了。别罗嗦了，快回家收拾行李，乘着还没天黑赶快动身。”',\n",
       " 'instruction': '给你一段话，帮我继续写下去',\n",
       " 'output': '张石头将信将疑，又实在细胳臂扭不过大腿，只得咬牙决定赌上一把，“好吧，既然少爷你执意要去，小的就陪你走上这一趟。不过将来老爷如果追究起来，少爷你可要给小的说几句好话。”张大少爷咧嘴一笑，安慰道：“放心吧，如果老头子真的把我们抓住了，少爷我全抗下来，不关你事。”\\n……\\n好不容易说服张石头同行，当下张大少爷和张石头一主一仆两个活宝立即行动起来，先是翻墙回家分头行动，张石头摸到张大少爷房间里收拾衣服行李，张大少爷则通过记忆找出前任张大少爷偷配的银库钥匙摸进张老财的书房，打开张老财用来收藏贵重财物的密库，搜刮出厚厚一大叠银票和满满一大包金叶子金元宝，又装了不少极其贵重的珠宝，这才偷偷摸回自己的房间与张石头会合。\\n刚进门，张石头就举起一个包裹说岛：“少爷，东西都准备好了，按你的吩咐，你和我都只带了一套换洗的衣服，还有一点零用的碎银子。”张大少爷点点头，又让张石头拿出房中已经积满灰尘的笔墨纸砚，吩咐道：“石头，快，给我家老头子留一封信，就说少爷我要进京去考状元，叫他不用担心，少爷考上状元就会回家或者接他去京城享福。还有，告诉老头子，少爷我是坐船进京，叫他不要去追。”'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "text_data = loaded_dataset['train'][0]['input']\n",
    "# 创建 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=r'./TOKENIZER')  # 可以根据需要选择基础模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393cc780d5944ad9bbfa099bfac2352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = loaded_dataset\n",
    "\n",
    "# 应用tokenizer到数据集\n",
    "def apply_tokenizer(example):\n",
    "    return tokenizer(example['input'], example['instruction'], example['output'], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(apply_tokenizer, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']),\n",
       " {'input_ids': tensor([17312,   231, 17739,   114, 10310,   119, 33232,   227, 17312,   231,\n",
       "          17739,   114, 20015,   228,   171,   120,   234,   161,   240,   109,\n",
       "          20015,   105, 28156,   254, 32014, 22887,   239,   163,   230,   115,\n",
       "          21410, 46237,   251, 32573,   246,   162,   110,    94, 17312,   231,\n",
       "          46237,   112, 22522,   234,   171,   120,   234, 28156,   254,   163,\n",
       "            253,   111, 13783,   112, 21410, 20998,    96, 36365,   112, 22887,\n",
       "            109, 32432,   110,   163,   119,   237, 38184,   223, 49035,   118,\n",
       "          12859,   228,   161,   246,   112,   164,   100,   240,   171,   120,\n",
       "            234, 28156,   254, 32014, 22887,   239,   163,   230,   115, 20998,\n",
       "          42062,   114,   223,   163,   225,   255, 33699,   241,   165,   241,\n",
       "            223,   171,   120,   234,   165,   120,   241, 27950,   101, 34402,\n",
       "            241,   171,   120,   248,   447,   250, 19526,   254, 46349,   111,\n",
       "          46349,   111,   171,   120,   234,   163,   228,   232,   163,   239,\n",
       "          21253,   224,    96, 10310,   103, 10310,   104,   165,   105,   253,\n",
       "            163,   100,   222,   161,   226,   123,   165,   243,   123, 36181,\n",
       "            245, 17312,   231, 13783,   248,   162,   120,   224, 12859,   106,\n",
       "            161,   243,   232,   171,   120,   253, 19526,   254, 36685,   224,\n",
       "            162,   252,   250, 31660,   164,   122,   230, 36310,   163,   103,\n",
       "            251, 28839,   101, 32573,   247, 10310,   103, 10310,   112,   162,\n",
       "            116,   227,   161,   253,   236, 34932,   234,   171,   120,   234,\n",
       "          17312,   231,   162,   110,    94, 17312,   231, 20998,   107, 47797,\n",
       "            121,   164,   106,   101, 26344,   108, 32573,   247, 20046,   230,\n",
       "            162,   120,   224, 12859,   106, 21410,   161,   103,   111, 36685,\n",
       "            229,   171,   120,   253, 32573,   247, 20046,   230, 25001,   121,\n",
       "          21410, 17312,   118, 27670,   248,   162,   242,   122, 28839,   101,\n",
       "            165,   251,    95, 30298,   235,   171,   120,   234, 49694,   122,\n",
       "          34402,   241, 19526,   254, 46349,   111,   165,   242,   247, 32573,\n",
       "            229,   171,   120,   234, 20015,    98, 28938,   236, 22887,   109,\n",
       "          20998,   103,   162, 46788,   163,   251,   222, 31660, 10310,   103,\n",
       "            165,   119,   226,   164,   226,   116,   161,   102,   228, 32573,\n",
       "            229, 10310,   233, 39355,   232,   164,   122,   230, 36310,   171,\n",
       "            120,   253,   447,   251,   198, 28156,   254, 32014, 22887,   239,\n",
       "            163,   230,   115, 21410, 46237,   251, 20998,    98, 20998,    98,\n",
       "          32849,   121, 33699,   241, 40792, 17358,   223, 22522,   111,   171,\n",
       "            120,   234, 12859,   234, 39355,   223, 49035,   118, 13783,   112,\n",
       "          32573,   246,   162,   110,    94,   164,   106,   101, 26344,   108,\n",
       "          32003,   223,   161,   102,   228, 21410, 28156,   254,   163,   253,\n",
       "            111, 13783,   112, 49694,   122, 17739,   235, 27950,   101, 33232,\n",
       "            225, 33768,   254,   162,   107,   242,   171,   120,   234, 38834,\n",
       "          32573,   229, 20015,   242,   163,   119,   228, 32003,   225,   164,\n",
       "            247,   239, 12859,   228, 31660, 10310,   233, 28938,   236,   171,\n",
       "            120,   234, 28156,   254,   163,   253,   111, 13783,   112, 32573,\n",
       "            246, 42468,   161,   241,   255, 10310,   100,   163,   251,   222,\n",
       "            164,   226,   116, 46237,   112, 34402,   241,   171,   120,   248,\n",
       "            447,   250, 22887,   239,   163,   230,   115,   171,   120,   234,\n",
       "          38834, 42468, 22887,   237, 21410,   162,   233,    99,   163,   251,\n",
       "            222, 19526,   254,   171,   120,   234, 20998,   103, 42468, 32573,\n",
       "            247, 12859,   233, 20998,   107, 38834, 42468, 28156,   222,   163,\n",
       "            236,   102,   163,   119,   247, 19526,   254, 31660,   162,   106,\n",
       "            113, 46237,   251,   171,   120,   234, 30585,   106, 22755,   239,\n",
       "            163,   119,   100,   163,   119,   255, 37863,   247, 10310,   233,\n",
       "          43889,   119]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'labels': tensor([28156,   254,   163,   253,   111, 13783,   112, 49546, 46479,    94,\n",
       "          49546,   163,   244,   239,   171,   120,   234, 20998,   230, 22522,\n",
       "            252, 28839,   101,   163,   119,   228, 47797,   111,   164,   229,\n",
       "            224, 33699,   255, 38834, 32573,   229, 32014,   164,   227,   123,\n",
       "            171,   120,   234, 20998,   103, 36181,   245,   161,   240,   105,\n",
       "          31965,   247, 37863,   111, 22522,   248,   164,   113,   234, 41468,\n",
       "          31660,   162,   232,   232,   171,   120,   234,   447,   250, 25001,\n",
       "            121, 28938,   100,   171,   120,   234, 33768,    95, 47078,   114,\n",
       "          22887,   239,   163,   230,   115, 19526,   254, 33699,   100, 35707,\n",
       "            237, 17358,   223, 43889,   119,   171,   120,   234, 22887,   237,\n",
       "          21410, 22887,   109,   165,   247,   103, 19526,   254,   164,   113,\n",
       "            108, 41468, 32573,   247, 31660,   164,   114,   253, 16764, 38834,\n",
       "          32573,   229, 49546, 30266,    98, 32003,   223,   163,   230,   115,\n",
       "          36685,   224,   162,   252,   250,   164,  4204,   163,   102,   114,\n",
       "            164,   113,   115, 30266,    98,   171,   120,   234, 22887,   239,\n",
       "            163,   230,   115, 19526,   254, 20998,   107, 17358,   223,   163,\n",
       "            119,   247, 22887,   237, 21410, 46237,   112, 49035,   254, 20998,\n",
       "             98, 25001,   121, 46237,   251, 16764,   447,   251, 28156,   254,\n",
       "          32014, 22887,   239,   163,   230,   115,   161,   240,   100,   161,\n",
       "            246,   112, 31660,   163,   105,   239,   171,   120,   234, 22522,\n",
       "            231,   162,   227,   108, 34402,   241,   171,   120,   248,   447,\n",
       "            250,   162,   242,   122, 33232,   225, 28938,   100,   171,   120,\n",
       "            234, 36685,   224,   162,   252,   250, 32003,   223, 13783,   112,\n",
       "          36310, 40367,   253, 21410,   162,   232,   232, 22755,   239, 20015,\n",
       "            105,   162,   232,   241, 19526,   237, 12859,   228,   171,   120,\n",
       "            234, 22887,   239,   163,   230,   115, 22755,   239, 17739,   101,\n",
       "            162,   232,   245, 10310,   233, 30266,    98,   171,   120,   234,\n",
       "          38834, 17739,   111, 19526,   254, 12859,   233, 16764,   447,   251,\n",
       "            198,  7398,   198, 25001,   121, 38834, 22522,   117, 23626,   241,\n",
       "          46237,   112, 17312,   235, 28156,   254,   163,   253,   111, 13783,\n",
       "            112, 28938,   234, 26193,   234,   171,   120,   234, 37605,   241,\n",
       "          10310,   233, 28156,   254, 32014, 22887,   239,   163,   230,   115,\n",
       "            161,   240,   234, 28156,   254,   163,   253,   111, 13783,   112,\n",
       "          31660, 10310,   119, 31660, 20015,   228, 10310,    97, 10310,   103,\n",
       "            162,   112,   119, 22522,   251, 44165,   233, 39355,   111, 26193,\n",
       "            234, 27950,   101,   164,   113,   115, 30266,    98,   171,   120,\n",
       "            234, 17739,   230, 42468,   163,   123,   119,   161,    95,   247,\n",
       "          32368,   252, 22522,   114, 26344,   228, 13783,   112, 26193,   234,\n",
       "          27950,   101,   171,   120,   234, 28156,   254,   163,   253,   111,\n",
       "          13783,   112,   162,   239,   116, 26344,   108, 28156,   254, 32014,\n",
       "          22887,   239,   163,   230,   115, 22755,   123, 29785,   112, 34932,\n",
       "            234,   162,   242, 35050,   233,   122, 26193,    96, 17312,   235,\n",
       "          26193,   234, 30266,   236,   171,   120,   234, 28156,   254, 32014,\n",
       "          22887,   239,   163,   230,   115, 26344,   247, 34460,   248, 32573,\n",
       "            229,   164,   106,   108, 33232,   228, 33699,   122, 49035,   118,\n",
       "          30298,   235, 20015,   119, 28156,   254, 32014, 22887,   239,   163,\n",
       "            230,   115,   161,   223,   115,   165,   227,   235, 21410,   165,\n",
       "            241,   114, 41753,   241,   165,   240,    98, 44293,   247,   162,\n",
       "            239,   116, 32573,   249, 28156,   254, 32003,   223,   164,   112,\n",
       "             95, 21410, 20046,    99, 22755,   123,   171,   120,   234, 33699,\n",
       "            241, 28156,   222, 28156,   254, 32003,   223,   164,   112,    95,\n",
       "          18796,   101])})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0].keys(),tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\",'attention_mask', \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][600]['labels'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infini-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tot.infini_llama import *\n",
    "from transformers import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaConfig, LlamaForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "config = LlamaConfig()\n",
    "config.hidden_size=256\n",
    "config.intermediate_size=172\n",
    "config.max_position_embeddings=256\n",
    "config.num_attention_heads=1\n",
    "config.num_hidden_layers=1\n",
    "config.num_key_value_heads=1\n",
    "config.rope_theta=10.0\n",
    "config.vocab_size=200\n",
    "\n",
    "# 配置模型\n",
    "model = LlamaForCausalLM(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torch\n",
    "\n",
    "# 创建DataLoader对象\n",
    "train_loader = DataLoader(tokenized_dataset['train'], batch_size=32, shuffle=True)\n",
    "\n",
    "next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练循环示例\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # 假设batch是一个包含多个样本的元组\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # 这里添加模型的前向传播、计算损失、反向传播和优化器步骤\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "dataset = TextDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "model = EncoderDecoderModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 训练模型\n",
    "model.train()\n",
    "for epoch in range(3):  # 简单的示例，只训练3个epoch\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        outputs = model(input_ids=inputs, decoder_input_ids=inputs[:, :-1])\n",
    "        loss = loss_fn(outputs.view(-1, outputs.shape[-1]), inputs[:, 1:].view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
